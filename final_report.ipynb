{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#PLOT & MATH LIBS\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective of this notebook is to illustrate several simple techniques which improve the predictive performance of Parkinson's Disease progression (PD) on a population of PD patients. The progression of PD is measured using the PPMI database (https://ida.loni.usc.edu/login.jsp) which consists of UPDRS-III composite scores from 3,812 participants over 13 years. More specifically, we focus on the 3rd subtest of UPDRS-III, which is composed of in clinic motor examinations on patients and controls. This is widely considered (and some quick tests support this claim) to be the more statistically stable of the 4 sub-tests given its mechanical in-person nature. Our results show that a few simple techniques can significantly reduce model variance relative to the predicted outcome, without model selection, tuning, or parameterization. Additionally, we also show another feasible approach alongside ideas for follow up research. \n",
    "\n",
    "***Mention: ***\n",
    "1. The need for improved prediction processes\n",
    "2. add in the State model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Pre-process data set\n",
    "UPDRS3 = \"data/MDS-UPDRS_Part_III_10Jun2024.csv\"\n",
    "patient_status = \"data/Participant_Status_03Jun2024.csv\"\n",
    "\n",
    "df3 = pd.read_csv(UPDRS3)\n",
    "df_pat_stat = pd.read_csv(patient_status) #patient status data\n",
    "df3 = df3.dropna(subset=['NP3TOT']).reset_index() # will keep for now, might need to include nans\n",
    "df3['INFODT'] = pd.to_datetime(df3['INFODT'], format=\"%m/%Y\") #reformat INFODT (Assesment Date) to date-time objects\n",
    "df3['PDSTATE'] =  df3['PDSTATE'].fillna(\"None\")\n",
    "df3 = df3[[\"PATNO\", \"EVENT_ID\", \"INFODT\", \"PDSTATE\", \"PAG_NAME\", \"NP3TOT\"]]\n",
    "\n",
    "desired_cols_df_pat = {'PATNO', 'COHORT', 'ENROLL_STATUS'}\n",
    "pat_filtered = df_pat_stat.drop(columns=set(df_pat_stat.columns) - desired_cols_df_pat)\n",
    "df3_full = pd.merge(df3, pat_filtered, on=\"PATNO\")\n",
    "df3_full = df3_full[df3_full['ENROLL_STATUS'].isin(['Enrolled', 'Withdrew', 'Complete'])]\n",
    "df3_full.drop(columns=['ENROLL_STATUS'], inplace=True)\n",
    "df3_full = df3_full.sort_values(['PATNO', 'INFODT'])\n",
    "\n",
    "# Partition our data sets\n",
    "upd3_control = df3_full[df3_full['COHORT'] == 2]\n",
    "upd3_PD = df3_full[df3_full['COHORT'] == 1]\n",
    "upd3_PD_nan = upd3_PD[(upd3_PD['PDSTATE'] != 'ON') & (upd3_PD['PDSTATE'] != 'OFF') & (upd3_PD['PAG_NAME'] != 'NUPDR3OF') & (upd3_PD['PAG_NAME'] != 'NUPDR3ON')].reset_index(drop=True)\n",
    "upd3_PD_off = upd3_PD[(upd3_PD['PDSTATE'] == 'OFF') | (upd3_PD['PAG_NAME'] == 'NUPDR3OF')].reset_index(drop=True)\n",
    "upd3_PD_on = upd3_PD[(upd3_PD['PDSTATE'] == 'ON') | (upd3_PD['PAG_NAME'] == 'NUPDR3ON')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Selection and Logic\n",
    "As a preamble, this research turned out to be less focused on model selection and parameterization than initially expected. The models themselves are only as good as the data they train on (mostly), and here the discovery is that a clear framing of the PD progression problem as a time-series forcasting problem does a surprising amount of work. On that note, the models used here are placeholders. I selected Random-Forest-Regression (RFR) primarily because my supervisor (who's vastly more knowledgeable and skilled than I) recommended it. I decided to use Ridge-Regression as the inclusion of a linear model to compare to the more complex RFR could add some clues for future research on optimal model selection (though not a true comparison given the lack of a structure hyposthesis test). Additionally, Ridge-Regression could be more robust to the amount of noise inherent in the UPDRS-III dataset due to the slope penalty vs. simple Linear-Regression. \n",
    "\n",
    "**Placeholder Models**\n",
    "\n",
    "- Random Forest Regression, default 5 nodes\n",
    "- Ridge Regression\n",
    "\n",
    "##### Data Preparation Process\n",
    "\n",
    "The objective is to create rows in a wide-dataframe with the following format: `t0 | u0 | t1 | u1 | ... | t_i | u_i`, where `t_i` is the date where score `u_i` was measured. Each row represents the `[date | score]` entries for a given patient. The following algorithm performs this task:\n",
    "\n",
    "1. Given the non-uniform number of respective patient entries (some have 1, others more than 20) and to retain as much data as possible for our models we decided to bound the number of observations per patient to 3 (`i = 3`), which allowed us to keep patients with at least 3 and those with many more entries. So each patient's entries will be: `t0 | u0 | t1 | u1 | t_2 | u_2`\n",
    "\n",
    "   1.  This is done by using the individual patients entries to create a calendar, which is then segmented into 3 non-overlapping intervals, each greater than 6 months. The 6 month limit was chosen as it allows the disease to progress and therefore reduces overall measurement noise. The interval time delta is calculated as (end - start) / 3, where both end and start are drawn from the patients first and last visit INFODT (or date of examination). The patients individual calendar is then created by iteratively adding this time delta to the start date. This creates 4 dates, and thereby 3 intervals.\n",
    "\n",
    "2. From here, the patients 3 entries, `u0 | u1| u2` are produced as follows (through the functions `process_mean_intervals` and `process_rand_intervals`): \n",
    "   1. For the random basline, each of the 3 entries are randomly drawn from it's respective interval [Note 1]\n",
    "   2. The improved model takes the mean UPDRS-III score over the respective intervals as its entries\n",
    "\n",
    "3. For both methods, the date attached to these intervals,`t_i`, is the median date of each interval, which is calculated by adding 1/2 the time delta to each of the intervals start dates.\n",
    "4. Finally, the returned dataframes are converted to a wide format such that each row represents the 3 selected (or mean aggregated) INFODT-score combinations for a given PATNO.\n",
    "\n",
    "\n",
    "**Note 1**: It's also possible to randomly select 3 sequential patient entries such that they are taken at least 6 months from each other. But this wouldn't allow us to best measure the improvment in model performance using the mean over intervals.\n",
    "\n",
    "**Note 2**: (Rough) Function documentation for helper functions\n",
    "- `median_date_calc` : returns the median date of a grouped object\n",
    "- `interpolate_same_month` : Returns the specified computation (or random selection) on a grouped object. The grouping is done to filter same PATNO's and equivalent dates for the same PATNO, which for this analysis adds noise\n",
    "- `pivot_wide` : Returns the passed dataframe in wide format, some pre-processing is done in the `process_mean_intervals` and `process_rand_intervals` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing and Helper Functions\n",
    "def median_date_calc(date_group: pd.DataFrame, interval: pd.Timedelta) -> list:\n",
    "    dates = date_group['INFODT'].tolist()\n",
    "    date1 = dates[0] + interval / 2\n",
    "    date2 = dates[1] + interval / 2\n",
    "    date3 = dates[2] + interval / 2\n",
    "    return [(date1-date1).days, (date2-date1).days, (date3-date1).days]\n",
    "\n",
    "def interpolate_same_month(df: pd.DataFrame, method = 'rand') -> pd.DataFrame:\n",
    "    # Select maximum score from same month measurements\n",
    "    temp_df = df.copy()\n",
    "    temp_df['YEAR_MONTH'] = temp_df['INFODT'].dt.to_period('M')\n",
    "\n",
    "    #takes random selctions/maximum/minimum/mean of values which share same month and year\n",
    "    if method == 'rand':\n",
    "        sample = temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].apply(lambda x: x.sample(1)).reset_index()\n",
    "        result = pd.merge(temp_df, sample, on=['PATNO', 'YEAR_MONTH', 'NP3TOT'])\n",
    "\n",
    "        result.drop(columns=['YEAR_MONTH', 'level_2'], inplace=True)\n",
    "        return result.drop_duplicates(subset=['PATNO', 'INFODT']).reset_index(drop=True)\n",
    "\n",
    "    elif method == 'min':\n",
    "        #result = pd.merge(temp_df, temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].min(), on=['PATNO', 'YEAR_MONTH', 'NP3TOT'])\n",
    "        temp_df['NP3TOT'] = temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].transform('min')\n",
    "    elif method == 'max':\n",
    "        #result = pd.merge(temp_df, temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].max(), on=['PATNO', 'YEAR_MONTH', 'NP3TOT'])\n",
    "        temp_df['NP3TOT'] = temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].transform('max')\n",
    "    else:\n",
    "        #result = pd.merge(temp_df, temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].mean(), on=['PATNO', 'YEAR_MONTH', 'NP3TOT'])\n",
    "        temp_df['NP3TOT'] = temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].transform('mean')\n",
    "\n",
    "    \n",
    "    temp_df.drop(columns=['YEAR_MONTH'], inplace=True)\n",
    "    temp_df.reset_index(drop=True, inplace=True)\n",
    "    return temp_df.drop_duplicates(subset=['PATNO', 'INFODT'])\n",
    "\n",
    "def pivot_wide(df: pd.DataFrame, cols: int = 3) -> pd.DataFrame:\n",
    "\n",
    "    df['time_index'] = (\n",
    "            df.groupby('PATNO')['INFODT']\n",
    "            .rank(method='first')\n",
    "            .astype(int)-1\n",
    "        ) \n",
    "    \n",
    "    time_df = df[['PATNO', 'INFODT', 'time_index']]\n",
    "\n",
    "    score_wide = df.pivot(\n",
    "        index='PATNO',\n",
    "        columns='time_index',\n",
    "        values='NP3TOT'\n",
    "    ).reset_index()\n",
    "\n",
    "    time_wide = time_df.pivot(\n",
    "        index='PATNO',\n",
    "        columns=\"time_index\",\n",
    "        values='INFODT'\n",
    "    ).reset_index()\n",
    "\n",
    "    score_wide.columns = ['PATNO'] + [f'u{i}' for i in range(cols)] # resets score column names correctly\n",
    "    time_wide.columns = ['PATNO'] + [f't{i}' for i in range(cols)] # resets time_index column names accordingly\n",
    "\n",
    "    merged = pd.merge(score_wide, time_wide, on='PATNO')\n",
    "    merged.drop(columns='PATNO', inplace=True)\n",
    "\n",
    "    new_cols = []\n",
    "    for i in range(cols): # re order columns for ML piplines\n",
    "        new_cols.append(f't{i}')\n",
    "        new_cols.append(f'u{i}')\n",
    "\n",
    "    merged = merged[new_cols]\n",
    "\n",
    "    return merged.drop(columns='t0')\n",
    "\n",
    "\n",
    "# keep_small_intervals flag allows us to keep some entries with <6mo intervals\n",
    "# assumes process_same_dates has already been calles\n",
    "def process_mean_intervals(df: pd.DataFrame, blocks: int = 3, keep_small_intervals = True) -> pd.DataFrame:\n",
    "    patnos = df['PATNO'].unique().tolist()\n",
    "    result = pd.DataFrame(columns=['PATNO', 'INFODT', 'NP3TOT'])\n",
    "    for patno in patnos:\n",
    "        df_pat = df[df['PATNO'] == patno]\n",
    "        start = df_pat['INFODT'].min()\n",
    "        end = df_pat['INFODT'].max()\n",
    "        diff = end - start\n",
    "\n",
    "        if (diff < pd.Timedelta(days=180)) or (df_pat.index.size < 3):\n",
    "            continue\n",
    "\n",
    "        interval = diff // blocks\n",
    "\n",
    "        # drop small intervals < 6mo\n",
    "        if not keep_small_intervals and interval < pd.Timedelta(days=180):\n",
    "            continue\n",
    "\n",
    "        group_counts = df_pat.groupby(pd.Grouper(key='INFODT', freq= f'{interval.days+1}D'))['NP3TOT'].count()\n",
    "        temp = df_pat.groupby(pd.Grouper(key='INFODT', freq= f'{interval.days+1}D'))['NP3TOT'].mean().reset_index()\n",
    "        temp['group_counts'] = group_counts.values\n",
    "        # checks for Failed Time Group Function \n",
    "        if temp['NP3TOT'].isnull().values.any():\n",
    "            continue\n",
    "        \n",
    "        dates = median_date_calc(df_pat.groupby(pd.Grouper(key='INFODT', freq= f'{interval.days}D'))['NP3TOT'].mean().reset_index(), interval)\n",
    "\n",
    "        # this logic will create some entries with < 6mo intervals, but this is only due to the abover grouper function using a slightly different calendar\n",
    "        temp['INFODT'] = dates\n",
    "        temp['PATNO'] = patno\n",
    "\n",
    "        result = pd.concat([result, temp], ignore_index=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "def process_random_intervals(df: pd.DataFrame, blocks: int = 3, keep_small_intervals = True) -> pd.DataFrame:\n",
    "    patnos = df['PATNO'].unique().tolist()\n",
    "    patnos = df['PATNO'].unique().tolist()\n",
    "    result = pd.DataFrame(columns=['PATNO', 'INFODT', 'NP3TOT'])\n",
    "    for patno in patnos:\n",
    "        df_pat = df[df['PATNO'] == patno]\n",
    "        start = df_pat['INFODT'].min()\n",
    "        end = df_pat['INFODT'].max()\n",
    "        diff = end - start\n",
    "\n",
    "        if (diff < pd.Timedelta(days=180)) or (df_pat.index.size < 3):\n",
    "            continue\n",
    "\n",
    "        interval = diff // blocks\n",
    "\n",
    "        # drop small intervals < 6mo\n",
    "        if not keep_small_intervals and interval < pd.Timedelta(days=180):\n",
    "            continue\n",
    "\n",
    "        temp = (\n",
    "            df_pat.groupby(pd.Grouper(key='INFODT', freq=f'{interval.days+1}D'))['NP3TOT']\n",
    "            .apply(lambda x: x.sample(n=1, random_state=1) if len(x) > 0 else None)\n",
    "            .dropna()  # Drop groups where no sample was taken (i.e., empty groups)\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # checks for Failed Time Group Function // when the groupby function can't return 3 groups (like it should)\n",
    "        if temp.index.size != 3:\n",
    "            #print(\"Failed Time Group Function\")\n",
    "            continue\n",
    "\n",
    "        dates = median_date_calc(df_pat.groupby(pd.Grouper(key='INFODT', freq= f'{interval.days}D'))['NP3TOT'].mean().reset_index(), interval)\n",
    "\n",
    "        # this logic will create some entries with < 6mo intervals, but this is only due to the abover grouper function using a slightly different calendar\n",
    "        temp['INFODT'] = dates\n",
    "        temp['PATNO'] = patno\n",
    "        temp.drop(columns=['level_1'], inplace=True)\n",
    "\n",
    "        result = pd.concat([result, temp], ignore_index=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"results.png\" alt=\"isolated\" width=\"800\" class = \"center\"/>\n",
    "![alt text](results.png \"Tabulated Results, all values in RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3f/j_4mwg8n6c137n0zbqfb_rbc0000gn/T/ipykernel_7171/1375517764.py:151: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result = pd.concat([result, temp], ignore_index=True)\n",
      "/var/folders/3f/j_4mwg8n6c137n0zbqfb_rbc0000gn/T/ipykernel_7171/1375517764.py:151: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result = pd.concat([result, temp], ignore_index=True)\n",
      "/var/folders/3f/j_4mwg8n6c137n0zbqfb_rbc0000gn/T/ipykernel_7171/1375517764.py:151: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result = pd.concat([result, temp], ignore_index=True)\n",
      "/var/folders/3f/j_4mwg8n6c137n0zbqfb_rbc0000gn/T/ipykernel_7171/1375517764.py:151: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result = pd.concat([result, temp], ignore_index=True)\n",
      "/var/folders/3f/j_4mwg8n6c137n0zbqfb_rbc0000gn/T/ipykernel_7171/1375517764.py:109: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result = pd.concat([result, temp], ignore_index=True)\n",
      "/var/folders/3f/j_4mwg8n6c137n0zbqfb_rbc0000gn/T/ipykernel_7171/1375517764.py:109: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result = pd.concat([result, temp], ignore_index=True)\n",
      "/var/folders/3f/j_4mwg8n6c137n0zbqfb_rbc0000gn/T/ipykernel_7171/1375517764.py:109: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result = pd.concat([result, temp], ignore_index=True)\n",
      "/var/folders/3f/j_4mwg8n6c137n0zbqfb_rbc0000gn/T/ipykernel_7171/1375517764.py:109: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result = pd.concat([result, temp], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Model testing function and pipeline\n",
    "\n",
    "# Imports\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipline Data Sets\n",
    "nan = interpolate_same_month(upd3_PD_nan, 'rand')\n",
    "off = interpolate_same_month(upd3_PD_off, 'rand')\n",
    "on = interpolate_same_month(upd3_PD_on, 'rand')\n",
    "control = interpolate_same_month(upd3_control, 'rand')\n",
    "\n",
    "#BASELINES RANDOM\n",
    "base_nan_rand = np.std(nan['NP3TOT'])\n",
    "base_off_rand = np.std(off['NP3TOT'])\n",
    "base_on_rand  = np.std(on['NP3TOT'])\n",
    "base_control_rand = np.std(control['NP3TOT'])\n",
    "\n",
    "PD_nan_rand = pivot_wide(process_random_intervals(nan, 3, keep_small_intervals=True))\n",
    "PD_off_rand = pivot_wide(process_random_intervals(off, 3, keep_small_intervals=True))\n",
    "PD_on_rand = pivot_wide(process_random_intervals(on, 3, keep_small_intervals=True))\n",
    "control_rand = pivot_wide(process_random_intervals(control, 3, keep_small_intervals=True))\n",
    "\n",
    "#DE-NOISED\n",
    "nan_mean = interpolate_same_month(upd3_PD_nan, 'mean')\n",
    "off_mean = interpolate_same_month(upd3_PD_off, 'mean')\n",
    "on_mean = interpolate_same_month(upd3_PD_on, 'mean')\n",
    "control_mean = interpolate_same_month(upd3_control, 'mean')\n",
    "\n",
    "base_nan_mean = np.std(nan_mean['NP3TOT'])\n",
    "base_off_mean = np.std(off_mean['NP3TOT'])\n",
    "base_on_mean  = np.std(on_mean['NP3TOT'])\n",
    "base_control_mean = np.std(control_mean['NP3TOT'])\n",
    "\n",
    "PD_nan_mean = pivot_wide(process_mean_intervals(nan_mean, 3, keep_small_intervals=True))\n",
    "PD_off_mean = pivot_wide(process_mean_intervals(off_mean, 3, keep_small_intervals=True))\n",
    "PD_on_mean = pivot_wide(process_mean_intervals(on_mean, 3, keep_small_intervals=True))\n",
    "control_mean = pivot_wide(process_mean_intervals(control_mean, 3, keep_small_intervals=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Caller Functions\n",
    "def test_models(df:pd.DataFrame, base_line: float, identifier: str = \"DEFAULT\", folds = 5):\n",
    "    X = df.iloc[: , : len(df.columns)-1]\n",
    "    y = df.iloc[: , len(df.columns)-1 : ]\n",
    "\n",
    "    pipe_rf = Pipeline(steps=[('model', RandomForestRegressor())])\n",
    "    pipe_ridge = Pipeline(steps=[('model', Ridge())])\n",
    "    # .values will give the values in a numpy array (shape: (n,1))\n",
    "    # .ravel will convert that array shape to (n, ) (i.e. flatten it)\n",
    "    score_test_rf = -1 * cross_val_score(pipe_rf, X, y.values.ravel(), cv=folds, scoring=\"neg_root_mean_squared_error\")\n",
    "    score_test_ridge = -1 * cross_val_score(pipe_ridge, X, y.values.ravel(), cv=folds, scoring=\"neg_root_mean_squared_error\")\n",
    "\n",
    "    print(f\"Testing {identifier} Model\")\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(f\"STD(NP3TOT) {identifier}: {base_line}\")\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "    print(f\"Startings models, Simple Cross Validation, k = {folds}, VS std(UPDRS): \\n\")\n",
    "    print(\" Ridge Regression RMSE by fold: \", score_test_ridge)\n",
    "    print(\" Random Forest RMSE by fold: \", score_test_rf, \"\\n\")\n",
    "    print(f\" Ridge Regression mean RMSE: {score_test_ridge.mean()}\", '\\n', f\"Random Forest, default 5-nodes, mean RMSE: {score_test_rf.mean()}\")\n",
    "\n",
    "    return [score_test_rf.mean(), score_test_ridge.mean()]\n",
    "\n",
    "def compare_models(df_mean: pd.DataFrame, df_rand: pd.DataFrame, base_mean: float, base_rand: float):\n",
    "    mean = test_models(df_mean, base_mean, \"MEAN\")\n",
    "    rand = test_models(df_rand, base_rand, \"Random Selection\")\n",
    "\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Comparing Models\")\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(f\"Mean Model: {mean}\")\n",
    "    print(f\"Random Model: {rand}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thoughts on follow up research\n",
    "- Model Selection: Here we used only placeholder models with no parameterization. The next phase would be to research and test other models. \n",
    "- New Features: The only feature in our analysis is time. Medical imaging, Age, and the other 3 UPDRS-III scores are obvious candidates to include here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
