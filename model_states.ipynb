{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#PLOT & MATH LIBS\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import random as random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDRS3 = \"data/MDS-UPDRS_Part_III_10Jun2024.csv\"\n",
    "patient_status = \"data/Participant_Status_03Jun2024.csv\"\n",
    "\n",
    "df3 = pd.read_csv(UPDRS3)\n",
    "df_pat_stat = pd.read_csv(patient_status) #patient status data\n",
    "df3 = df3.dropna(subset=['NP3TOT']).reset_index() # will keep for now, might need to include nans\n",
    "df3['INFODT'] = pd.to_datetime(df3['INFODT'], format=\"%m/%Y\") #reformat INFODT (Assesment Date) to date-time objects\n",
    "df3['PDSTATE'] =  df3['PDSTATE'].fillna(\"None\")\n",
    "df3 = df3[[\"PATNO\", \"EVENT_ID\", \"INFODT\", \"PDSTATE\", \"PAG_NAME\", \"NP3TOT\"]]\n",
    "\n",
    "desired_cols_df_pat = {'PATNO', 'COHORT', 'ENROLL_STATUS'}\n",
    "pat_filtered = df_pat_stat.drop(columns=set(df_pat_stat.columns) - desired_cols_df_pat)\n",
    "df3_full = pd.merge(df3, pat_filtered, on=\"PATNO\")\n",
    "df3_full = df3_full[df3_full['ENROLL_STATUS'].isin(['Enrolled', 'Withdrew', 'Complete'])]\n",
    "df3_full.drop(columns=['ENROLL_STATUS'], inplace=True)\n",
    "df3_full = df3_full.sort_values(['PATNO', 'INFODT'])\n",
    "\n",
    "# Partition our data sets\n",
    "upd3_control = df3_full[df3_full['COHORT'] == 2]\n",
    "upd3_PD = df3_full[df3_full['COHORT'] == 1]\n",
    "# Data set of interest...\n",
    "upd3_PD_off_on = upd3_PD[(upd3_PD['PDSTATE'] == 'ON') | (upd3_PD['PDSTATE'] == 'OFF') | (upd3_PD['PAG_NAME'] == 'NUPDR3OF') | (upd3_PD['PAG_NAME'] == 'NUPDR3ON')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logic for same month entries\n",
    "- iterate through all dates selected per PATNO\n",
    "- if one belongs to the set of dates which have duplicate entries in compliment states -> \n",
    "  - append new entries to new dataframe which has that INFODT entry swapped with the compliment\n",
    "  - so maximum three new entry sets per patno\n",
    "- pivot and concat this dataframe to the original\n",
    "\n",
    "**Note:** This code is buggy, if errors continue please re-run the notebook (will fix on my own time in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_same_month(df: pd.DataFrame, method = 'rand') -> pd.DataFrame:\n",
    "    # Select maximum score from same month measurements\n",
    "    temp_df = df.copy()\n",
    "    temp_df['YEAR_MONTH'] = temp_df['INFODT'].dt.to_period('M')\n",
    "\n",
    "    #takes random selctions/maximum/minimum/mean of values which share same month and year\n",
    "    if method == 'rand':\n",
    "        sample = temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].apply(lambda x: x.sample(1)).reset_index()\n",
    "        result = pd.merge(temp_df, sample, on=['PATNO', 'YEAR_MONTH', 'NP3TOT'])\n",
    "\n",
    "        result.drop(columns=['YEAR_MONTH', 'level_2'], inplace=True)\n",
    "        return result.drop_duplicates(subset=['PATNO', 'INFODT']).reset_index(drop=True)\n",
    "\n",
    "    elif method == 'min':\n",
    "        #result = pd.merge(temp_df, temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].min(), on=['PATNO', 'YEAR_MONTH', 'NP3TOT'])\n",
    "        temp_df['NP3TOT'] = temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].transform('min')\n",
    "    elif method == 'max':\n",
    "        #result = pd.merge(temp_df, temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].max(), on=['PATNO', 'YEAR_MONTH', 'NP3TOT'])\n",
    "        temp_df['NP3TOT'] = temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].transform('max')\n",
    "    else:\n",
    "        #result = pd.merge(temp_df, temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].mean(), on=['PATNO', 'YEAR_MONTH', 'NP3TOT'])\n",
    "        temp_df['NP3TOT'] = temp_df.groupby(['PATNO', 'YEAR_MONTH'])['NP3TOT'].transform('mean')\n",
    "\n",
    "    \n",
    "    temp_df.drop(columns=['YEAR_MONTH'], inplace=True)\n",
    "    temp_df.reset_index(drop=True, inplace=True)\n",
    "    return temp_df.drop_duplicates(subset=['PATNO', 'INFODT'])\n",
    "\n",
    "\n",
    "def process_dates_caller(input: pd.DataFrame, entries: int = 3, same_month_flag = False) -> pd.DataFrame:\n",
    "    # helps maintain random states of dataframes to compare to baselines, default is to assume input is already interpolated\n",
    "    df = interpolate_same_month(input, 'rand') if same_month_flag else input\n",
    "\n",
    "    patnos = df['PATNO'].unique().tolist()\n",
    "    df = input.copy()\n",
    "    limit = pd.Timedelta(6*30, unit='D') # 180 days ~ 6mo\n",
    "    date_chunks = []\n",
    "\n",
    "    for id in patnos:\n",
    "\n",
    "        visits = df[df['PATNO'] == id]['INFODT'].tolist()\n",
    "        soln = []\n",
    "        lim = 0\n",
    "\n",
    "        if len(visits) < entries:\n",
    "            #print(f\"Not Enough Entries for ID: {id}\")\n",
    "            continue\n",
    "\n",
    "        sub_df = df[df['PATNO'] == id]\n",
    "        random.seed(10)\n",
    "        \"\"\"\n",
    "            Include same month check and processor here \n",
    "        \"\"\"\n",
    "        \n",
    "        while True:\n",
    "            soln = np.random.choice(visits, entries, replace=False)\n",
    "\n",
    "            if (soln[1] - soln[0]) > limit and (soln[2] - soln[1]) > limit:\n",
    "                break\n",
    "            if lim > 100:\n",
    "                #print(f\"No Possible Calendar Combination Found for ID: {id}\")\n",
    "                break\n",
    "            lim += 1\n",
    "        \n",
    "        # add additional same_month flag here? then double append the two rows?\n",
    "        if lim < 100:\n",
    "            for i in range(entries):\n",
    "                date_chunks.append([id, sub_df.loc[sub_df['INFODT'] == soln[i], 'INFODT'].values[0], sub_df.loc[sub_df['INFODT'] == soln[i], 'NP3TOT'].values[0], sub_df.loc[sub_df['INFODT'] == soln[i], 'PDSTATE'].values[0]])     \n",
    "\n",
    "    res = pd.DataFrame(date_chunks, columns=['PATNO', 'INFODT', 'score', 'PDSTATE'])\n",
    "\n",
    "    # Prepare dataframe for pivoting\n",
    "\n",
    "    res['time_delta'] = (\n",
    "        res.groupby('PATNO')['INFODT']\n",
    "        .transform(\n",
    "            lambda x: (x - x.min()) / np.timedelta64(30, 'D')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    res['time_index'] = (\n",
    "            res.groupby('PATNO')['INFODT']\n",
    "            .rank(method='first')\n",
    "            .astype(int)-1\n",
    "        ) \n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def pivot_wide_with_states(input: pd.DataFrame, cols: int = 3) -> pd.DataFrame:\n",
    "    \n",
    "    time_df = input[['PATNO', 'time_delta', 'time_index']]\n",
    "    \n",
    "    score_wide = input.pivot(\n",
    "        index='PATNO',\n",
    "        columns='time_index',\n",
    "        values='score'\n",
    "    ).reset_index()\n",
    "\n",
    "    time_wide = time_df.pivot(\n",
    "        index='PATNO',\n",
    "        columns=\"time_index\",\n",
    "        values='time_delta'\n",
    "    ).reset_index()\n",
    "\n",
    "    off_wide = input.pivot(\n",
    "        index='PATNO',\n",
    "        columns='time_index',\n",
    "        values='OFF'\n",
    "    ).reset_index()\n",
    "\n",
    "    on_wide = input.pivot(\n",
    "        index='PATNO',\n",
    "        columns='time_index',\n",
    "        values='ON'\n",
    "    ).reset_index()\n",
    "\n",
    "    score_wide.columns = ['PATNO'] + [f'u{i}' for i in range(cols)] # resets score column names correctly\n",
    "    time_wide.columns = ['PATNO'] + [f't{i}' for i in range(cols)] # resets time_index column names accordingly\n",
    "    off_wide.columns = ['PATNO'] + [f'off_{i}' for i in range(cols)] # resets time_index column names accordingly\n",
    "    on_wide.columns = ['PATNO'] + [f'on_{i}' for i in range(cols)] # resets time_index column names accordingly\n",
    "\n",
    "    #merged = pd.merge(score_wide, time_wide, on='PATNO') # use PATNO to merge\n",
    "    #print(merged)\n",
    "    #merged.drop(columns='PATNO', inplace=True) # drop PATNO, no longer needed\n",
    "\n",
    "    merged = pd.concat([score_wide, time_wide, off_wide, on_wide], axis=1)\n",
    "    merged.drop(columns='PATNO', inplace=True)\n",
    "\n",
    "\n",
    "    new_cols = []\n",
    "    for i in range(cols): # re order columns for ML piplines\n",
    "        new_cols.append(f't{i}')\n",
    "        new_cols.append(f'off_{i}')\n",
    "        new_cols.append(f'on_{i}')\n",
    "        new_cols.append(f'u{i}')\n",
    "\n",
    "    merged = merged[new_cols]\n",
    "\n",
    "    return merged\n",
    "\n",
    "def fill_same_month(og_df: pd.DataFrame, pre_proc: pd.DataFrame) -> pd.DataFrame:\n",
    "    #filters on duplicate dates within patient ids\n",
    "    same_mo_rows = og_df.groupby(['PATNO', 'INFODT']).filter(lambda x: len(x) > 1).index\n",
    "    same_mo_df = og_df.iloc[same_mo_rows].copy()\n",
    "    duplicate_patnos = same_mo_df['PATNO'].unique().tolist()\n",
    "    final_res = pd.DataFrame(columns=['PATNO', 'INFODT', 'score', 'PDSTATE', 'time_delta', 'time_index'])\n",
    "\n",
    "    offset = 0\n",
    "    for id in duplicate_patnos:\n",
    "\n",
    "        selected_dates = pre_proc[pre_proc['PATNO'] == id]['INFODT'].tolist()\n",
    "        duplicate_dates = same_mo_df[same_mo_df['PATNO'] == id]['INFODT'].tolist()\n",
    "\n",
    "        res_slice = pre_proc[pre_proc['PATNO'] == id].copy()\n",
    "        dup_slice = same_mo_df[same_mo_df['PATNO'] == id].copy()\n",
    "        temp = []\n",
    "        for date in selected_dates:\n",
    "            if date in duplicate_dates:\n",
    "                #selected date = date\n",
    "                selected_state = res_slice['PDSTATE'].values[0]\n",
    "                \n",
    "                temp.append([\n",
    "                    id, \n",
    "                    date, \n",
    "                    dup_slice[(dup_slice['INFODT'] == date) & (dup_slice['PDSTATE'] != selected_state)]['NP3TOT'].values[0],\n",
    "                    dup_slice[(dup_slice['INFODT'] == date) & (dup_slice['PDSTATE'] != selected_state)]['PDSTATE'].values[0]\n",
    "                    ])\n",
    "                \n",
    "                #print(temp)\n",
    "\n",
    "        if len(temp) > 0:\n",
    "            for entry in temp:\n",
    "                #print(res_slice[res_slice['INFODT'] != entry[1]][['PATNO', 'INFODT', 'score', 'PDSTATE']], entry[1])\n",
    "                temp_res = res_slice[res_slice['INFODT'] != entry[1]][['PATNO', 'INFODT', 'score', 'PDSTATE']].copy()\n",
    "                temp_res.loc[3] = entry\n",
    "                #sort by date to get back to the correct time index\n",
    "                temp_res.sort_values('INFODT', inplace=True)\n",
    "\n",
    "                #add in the necessary columns for future pivoting\n",
    "                temp_res['time_delta'] = (\n",
    "                temp_res.groupby('PATNO')['INFODT']\n",
    "                .transform(\n",
    "                    lambda x: (x - x.min()) / np.timedelta64(30, 'D')\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                temp_res['time_index'] = (\n",
    "                        temp_res.groupby('PATNO')['INFODT']\n",
    "                        .rank(method='first')\n",
    "                        .astype(int)-1\n",
    "                    ) \n",
    "                \n",
    "                # needed to add this due to my own stupidity\n",
    "                offset += 1 if len(temp) > 1 else offset\n",
    "\n",
    "                temp_res['PATNO'] = offset\n",
    "\n",
    "                #print(temp_res)\n",
    "\n",
    "                final_res = pd.concat([final_res, temp_res], axis=0, join='outer', ignore_index=True)\n",
    "\n",
    "    return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3f/j_4mwg8n6c137n0zbqfb_rbc0000gn/T/ipykernel_7196/1078792028.py:202: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_res = pd.concat([final_res, temp_res], axis=0, join='outer', ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t0</th>\n",
       "      <th>off_0</th>\n",
       "      <th>on_0</th>\n",
       "      <th>u0</th>\n",
       "      <th>t1</th>\n",
       "      <th>off_1</th>\n",
       "      <th>on_1</th>\n",
       "      <th>u1</th>\n",
       "      <th>t2</th>\n",
       "      <th>off_2</th>\n",
       "      <th>on_2</th>\n",
       "      <th>u2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14.233333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>96.366667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>24.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>103.433333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>115.633333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>58.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.266667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>40.633333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1877 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       t0  off_0  on_0    u0          t1  off_1  on_1    u1          t2  \\\n",
       "0     0.0      1     0  39.0   14.233333      1     0  34.0   96.366667   \n",
       "1     0.0      0     1  23.0   18.200000      0     1  29.0   24.333333   \n",
       "2     0.0      1     0  47.0  103.433333      0     1  43.0  115.633333   \n",
       "3     0.0      0     1  39.0    9.100000      1     0  48.0   58.800000   \n",
       "4     0.0      0     1  39.0   15.266667      0     1  38.0   40.633333   \n",
       "...   ...    ...   ...   ...         ...    ...   ...   ...         ...   \n",
       "1872  0.0      0     1   7.0    6.033333      0     1   5.0   12.166667   \n",
       "1873  0.0      0     1  10.0    6.133333      1     0  14.0   13.233333   \n",
       "1874  0.0      0     1  10.0    6.033333      1     0  13.0   12.166667   \n",
       "1875  0.0      1     0   8.0    6.033333      0     1  14.0   12.166667   \n",
       "1876  0.0      1     0   8.0    6.033333      1     0  13.0   12.166667   \n",
       "\n",
       "      off_2  on_2    u2  \n",
       "0         0     1  50.0  \n",
       "1         1     0  38.0  \n",
       "2         0     1  44.0  \n",
       "3         1     0  58.0  \n",
       "4         0     1   9.0  \n",
       "...     ...   ...   ...  \n",
       "1872      1     0  12.0  \n",
       "1873      0     1   9.0  \n",
       "1874      0     1  27.0  \n",
       "1875      0     1  27.0  \n",
       "1876      0     1  27.0  \n",
       "\n",
       "[1877 rows x 12 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = process_dates_caller(upd3_PD_off_on, 3, True)\n",
    "on_hot = pd.get_dummies(test['PDSTATE'], dtype=int)\n",
    "on_hot.drop(columns=['None'], inplace=True)\n",
    "PD_on_off_encoded = pd.merge(test, on_hot, left_index=True, right_index=True)\n",
    "res_on_off = pivot_wide_with_states(PD_on_off_encoded, 3)\n",
    "\n",
    "duplicates = fill_same_month(upd3_PD_off_on, test)\n",
    "one_hot_dup = pd.get_dummies(duplicates['PDSTATE'], dtype=int)\n",
    "one_hot_dup.drop(columns=['None'], inplace=True)\n",
    "dup_encoded = pd.merge(duplicates, one_hot_dup, left_index=True, right_index=True)\n",
    "dup_res = pivot_wide_with_states(dup_encoded, 3)\n",
    "dup_res.dropna(inplace=True)\n",
    "\n",
    "analytical_set = pd.concat([res_on_off, dup_res], axis=0, join='outer', ignore_index=True)\n",
    "\n",
    "analytical_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple stratified Cross Validation testing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "base_std = upd3_PD_off_on['NP3TOT'].std()\n",
    "folds = 5\n",
    "pipe_rf = Pipeline(steps=[('model', RandomForestRegressor())])\n",
    "pipe_ridge = Pipeline(steps=[('model', Ridge())])\n",
    "\n",
    "X = analytical_set.loc[:, analytical_set.columns != 'u2']\n",
    "y = analytical_set['u2']\n",
    "score_test_rf = -1 * cross_val_score(pipe_rf, X, y.values.ravel(), cv=folds, scoring=\"neg_root_mean_squared_error\")\n",
    "score_test_ridge = -1 * cross_val_score(pipe_ridge, X, y.values.ravel(), cv=folds, scoring=\"neg_root_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD(NP3TOT) : 13.651124300403424\n",
      " Ridge Regression RMSE by fold:  [10.92387054 10.51875047 11.35378206 11.70293454 10.40530289]\n",
      " Random Forest RMSE by fold:  [7.38287297 6.89445109 9.08069068 9.05595292 8.3397948 ] \n",
      "\n",
      " Ridge Regression mean RMSE: 10.980928100329532 \n",
      " Random Forest, default 5-nodes, mean RMSE: 8.150752491417943\n"
     ]
    }
   ],
   "source": [
    "print(f\"STD(NP3TOT) : {base_std}\")\n",
    "print(\" Ridge Regression RMSE by fold: \", score_test_ridge)\n",
    "print(\" Random Forest RMSE by fold: \", score_test_rf, \"\\n\")\n",
    "print(f\" Ridge Regression mean RMSE: {score_test_ridge.mean()}\", '\\n', f\"Random Forest, default 5-nodes, mean RMSE: {score_test_rf.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
